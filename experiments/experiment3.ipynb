{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "PROJECT_DIR = os.path.dirname(os.getcwd())\n",
    "if PROJECT_DIR not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_DIR)\n",
    "\n",
    "from chord_recognition.cache import HDF5Cache\n",
    "from chord_recognition.cnn import deep_auditory_v2\n",
    "from chord_recognition.dataset import ChromaDataset, prepare_datasource, undersample_dataset\n",
    "from chord_recognition.utils import standardize, one_hot\n",
    "from chord_recognition.train import get_weighted_random_sampler, Solver\n",
    "from chord_recognition.ann_utils import convert_annotation_matrix\n",
    "from chord_recognition.evaluate import plot_confusion_matrix\n",
    "from chord_recognition.predict import forward\n",
    "\n",
    "\n",
    "torch.manual_seed(2020)\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (14, 5)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = HDF5Cache(os.path.join(PROJECT_DIR, 'chroma_cache.hdf5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exp3\n",
    "# Balance datasets separately and concat them into single dataset.\n",
    "\n",
    "# Balance beatles dataset\n",
    "ds = prepare_datasource(('beatles',))\n",
    "dataset = ChromaDataset(\n",
    "    ds, window_size=8192, hop_length=4096,\n",
    "    cache=cache)\n",
    "\n",
    "sampling_strategy = {\n",
    "    0: 8000,\n",
    "    2: 8000,\n",
    "    4: 8000,\n",
    "    5: 8000,\n",
    "    7: 8000,\n",
    "    9: 8000,\n",
    "    11: 8000,\n",
    "    24: 8000,\n",
    "}\n",
    "beatles_X, beatles_y = undersample_dataset(\n",
    "    dataset,\n",
    "    sampling_strategy,\n",
    "    RANDOM_STATE)\n",
    "\n",
    "# Balance robbie_williams dataset\n",
    "ds = prepare_datasource(('robbie_williams',))\n",
    "dataset = ChromaDataset(\n",
    "    ds, window_size=8192, hop_length=4096,\n",
    "    cache=cache)\n",
    "\n",
    "sampling_strategy = {\n",
    "    0: 8000,\n",
    "    2: 8000,\n",
    "    5: 8000,\n",
    "    7: 8000,\n",
    "    9: 8000,\n",
    "    24: 5000,\n",
    "}\n",
    "robbie_williams_X, robbie_williams_y = undersample_dataset(\n",
    "    dataset,\n",
    "    sampling_strategy,\n",
    "    RANDOM_STATE)\n",
    "\n",
    "# Balance queen dataset\n",
    "ds = prepare_datasource(('queen',))\n",
    "dataset = ChromaDataset(\n",
    "    ds, window_size=8192, hop_length=4096,\n",
    "    cache=cache)\n",
    "\n",
    "sampling_strategy = {\n",
    "    2: 4500,\n",
    "}\n",
    "queen_X, queen_y = undersample_dataset(\n",
    "    dataset,\n",
    "    sampling_strategy,\n",
    "    RANDOM_STATE)\n",
    "\n",
    "# Get zweieck data\n",
    "ds = prepare_datasource(('zweieck',))\n",
    "dataset = ChromaDataset(\n",
    "    ds, window_size=8192, hop_length=4096,\n",
    "    cache=cache)\n",
    "\n",
    "zweieck_X = [xi for xi, _ in dataset]\n",
    "zweieck_y = [yi for _, yi in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat all datasets\n",
    "dataset = itertools.chain(\n",
    "    zip(beatles_X, beatles_y),\n",
    "    zip(robbie_williams_X, robbie_williams_y),\n",
    "    zip(beatles_X, beatles_y),\n",
    "    zip(queen_X, queen_y),\n",
    "    zip(zweieck_X, zweieck_y),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train/val keeping equal proportions of the each class\n",
    "# Now train and val datasets have equal class probability distributions.\n",
    "ds1, ds2 = itertools.tee(dataset)\n",
    "X = [s for s, _ in ds1]\n",
    "targets = [t for _, t in ds2]\n",
    "indices = np.arange(len(X))\n",
    "X_train, X_val, y_train, y_val, _, _ = train_test_split(\n",
    "    X, targets, indices, test_size=0.2, stratify=targets, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate TRAIN_MEAN, TRAIN_STD\n",
    "X_train_temp = np.hstack([sample.squeeze(0) for sample in X_train])\n",
    "TRAIN_MEAN = X_train_temp.mean(axis=1).reshape(-1, 1)\n",
    "TRAIN_STD = X_train_temp.std(axis=1).reshape(-1, 1)\n",
    "\n",
    "# Rescale inputs to have a mean of 0 and std of 1\n",
    "train_data = [(standardize(i, TRAIN_MEAN, TRAIN_STD), t) for i, t in zip(X_train, y_train)]\n",
    "val_data = [(standardize(i, TRAIN_MEAN, TRAIN_STD), t) for i, t in zip(X_val, y_val)]\n",
    "\n",
    "del X_train_temp, X_train, X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance the classes in each batch which hopefully helps the training.\n",
    "sampler = get_weighted_random_sampler(targets, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "\n",
    "# Split dataset into train/val datasets\n",
    "loader_train = DataLoader(\n",
    "    dataset=train_data,\n",
    "    batch_size=batch_size,\n",
    "    sampler=sampler,\n",
    "    pin_memory=True,\n",
    "    num_workers=0)\n",
    "loader_val = DataLoader(\n",
    "    dataset=val_data,\n",
    "    num_workers=0,\n",
    "    batch_size=batch_size)\n",
    "\n",
    "dataloaders = {\n",
    "    \"train\": loader_train,\n",
    "    \"val\": loader_val\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = deep_auditory_v2()\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "learning_rate = 1e-3\n",
    "epochs=128\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999))\n",
    "\n",
    "solver = Solver(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    dataloaders=dataloaders,\n",
    "    learning_rate=learning_rate,\n",
    "    trained_model_name=\"deep_auditory_v2_exp3.pth\",\n",
    "    epochs=epochs)\n",
    "solver.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "\n",
    "model = deep_auditory_v2(pretrained=True)\n",
    "model.eval()\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = (i for i,_ in loader_val)\n",
    "y_hat_matrix = forward(model, val_loader, device, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_matrix = y_hat_matrix.cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['figure.dpi'] = 80\n",
    "\n",
    "y_matrix = one_hot(y_val, 25)\n",
    "y_true = convert_annotation_matrix(y_matrix)\n",
    "\n",
    "y_pred = convert_annotation_matrix(y_hat_matrix)\n",
    "labels = dataset.chord_labels\n",
    "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "\n",
    "plot_confusion_matrix(cm, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
