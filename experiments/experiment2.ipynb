{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "PROJECT_DIR = os.path.dirname(os.getcwd())\n",
    "if PROJECT_DIR not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_DIR)\n",
    "\n",
    "from chord_recognition.cache import HDF5Cache\n",
    "from chord_recognition.cnn import deep_auditory_v2\n",
    "from chord_recognition.dataset import ChromaDataset, prepare_datasource, undersample_dataset\n",
    "from chord_recognition.utils import standardize\n",
    "from chord_recognition.train import get_weighted_random_sampler, Solver\n",
    "\n",
    "torch.manual_seed(2020)\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (14, 5)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_files = (\n",
    "    # zweieck\n",
    "    '09_-_Mr_Morgan',\n",
    "    '01_-_Spiel_Mir_Eine_Alte_Melodie',\n",
    "    '11_-_Ich_Kann_Heute_Nicht',\n",
    "    # queen\n",
    "    '14 Hammer To Fall',\n",
    "    '08 Save Me',\n",
    "    # robbie_williams\n",
    "    '11-Man Machine',\n",
    "    '01-Ghosts',\n",
    "    '11-A Place To Crash',\n",
    "    '08-Heaven From Here',\n",
    "    '09-Random Acts Of Kindness',\n",
    "    '05-South Of The Border',\n",
    ")\n",
    "ds = prepare_datasource(\n",
    "    ('zweieck', 'queen', 'robbie_williams'),\n",
    "    excluded_files=excluded_files)\n",
    "\n",
    "allowed_files = (\n",
    "    '06-Mr_Moonlight',\n",
    "    '06-Yellow_Submarine',\n",
    "    '03-I_m_Only_Sleeping',\n",
    "    '09-Penny_Lane',\n",
    "    '12-Wait',\n",
    "    '11-Do_You_Want_To_Know_A_Secret',\n",
    "    '12-A_Taste_Of_Honey',\n",
    "    '04-I_m_Happy_Just_To_Dance_With_You',\n",
    "    '03-If_I_Fell',\n",
    "    '10-I_m_Looking_Through_You',\n",
    "    '09-When_I_m_Sixty-Four',\n",
    "    '06-Till_There_Was_You',\n",
    "    '05-Octopus_s_Garden',\n",
    "    '03-All_My_Loving',\n",
    "    '05-And_I_Love_Her',\n",
    "    '02-All_I_ve_Got_To_Do',\n",
    "    '10-For_No_One',\n",
    "    '08-Because',\n",
    "    '06-She_s_Leaving_Home',\n",
    "    '04-Chains',\n",
    "    '10-Things_We_Said_Today',\n",
    "    '09-One_After_909',\n",
    "    '09-Girl',\n",
    "    '14-Run_For_Your_Life',\n",
    "    '04-Oh_Darling',\n",
    "    '04-Don_t_Bother_Me',\n",
    "    '06-I_Want_You_She_s_So_Heavy_',\n",
    "    '06-Tell_Me_Why',\n",
    ")\n",
    "beatles_ds = prepare_datasource(('beatles',), allowed_files=allowed_files)\n",
    "datasource = ds + beatles_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all avaiable datasets\n",
    "dataset = ChromaDataset(\n",
    "    datasource, window_size=8192, hop_length=4096, context_size=7,\n",
    "    cache=HDF5Cache(os.path.join(PROJECT_DIR, 'chroma_cache.hdf5')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206446 train samples, 51612 val samples\n"
     ]
    }
   ],
   "source": [
    "# Split dataset into train/val keeping equal proportions of the each class\n",
    "# Now train and val datasets have equal probability distributions of classes\n",
    "X = [s for s, _ in dataset]\n",
    "targets = [t for _, t in dataset]\n",
    "indices = np.arange(len(X))\n",
    "X_train, X_val, y_train, y_val, _, _ = train_test_split(\n",
    "    X, targets, indices, test_size=0.2, stratify=targets, random_state=RANDOM_STATE)\n",
    "\n",
    "print(\"{0} train samples, {1} val samples\".format(len(X_train), len(X_val)))\n",
    "del X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate TRAIN_MEAN, TRAIN_STD\n",
    "X_train_temp = np.hstack([sample.squeeze(0) for sample in X_train])\n",
    "TRAIN_MEAN = X_train_temp.mean(axis=1).reshape(-1, 1)\n",
    "TRAIN_STD = X_train_temp.std(axis=1).reshape(-1, 1)\n",
    "\n",
    "# Rescale inputs to have a mean of 0 and std of 1\n",
    "train_data = [(standardize(i, TRAIN_MEAN, TRAIN_STD), t) for i, t in zip(X_train, y_train)]\n",
    "val_data = [(standardize(i, TRAIN_MEAN, TRAIN_STD), t) for i, t in zip(X_val, y_val)]\n",
    "\n",
    "del X_train_temp, X_train, X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance the classes in each batch which hopefully helps the training.\n",
    "sampler = get_weighted_random_sampler(targets, y_train)\n",
    "del targets, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "\n",
    "# Split dataset into train/val dataloaders\n",
    "loader_train = DataLoader(\n",
    "    dataset=train_data,\n",
    "    batch_size=batch_size,\n",
    "    sampler=sampler,\n",
    "    pin_memory=True,\n",
    "    num_workers=0)\n",
    "loader_val = DataLoader(\n",
    "    dataset=val_data,\n",
    "    num_workers=0,\n",
    "    batch_size=batch_size)\n",
    "\n",
    "dataloaders = {\n",
    "    \"train\": loader_train,\n",
    "    \"val\": loader_val\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = deep_auditory_v2()\n",
    "learning_rate = 1e-3\n",
    "epochs=128\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999))\n",
    "\n",
    "solver = Solver(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    dataloaders=dataloaders,\n",
    "    learning_rate=learning_rate,\n",
    "    epochs=epochs)\n",
    "solver.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from chord_recognition.utils import one_hot\n",
    "from chord_recognition.ann_utils import convert_annotation_matrix\n",
    "from chord_recognition.evaluate import plot_confusion_matrix\n",
    "from chord_recognition.predict import forward\n",
    "\n",
    "device = torch.device('cpu')\n",
    "model = deep_auditory_v2(pretrained=True)\n",
    "model.eval()\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = (i for i,_ in loader_val)\n",
    "y_hat_matrix = forward(model, val_loader, device, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_matrix = y_hat_matrix.cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['figure.dpi'] = 80\n",
    "\n",
    "y_matrix = one_hot(y_val, 25)\n",
    "y_true = convert_annotation_matrix(y_matrix)\n",
    "\n",
    "y_pred = convert_annotation_matrix(y_hat_matrix)\n",
    "labels = dataset.chord_labels\n",
    "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "\n",
    "plot_confusion_matrix(cm, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
