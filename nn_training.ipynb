{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy, copy\n",
    "import os\n",
    "import pathlib\n",
    "from random import shuffle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, ConcatDataset, WeightedRandomSampler\n",
    "from livelossplot import PlotLosses\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from chord_recognition.augmentations import SemitoneShift, DetuningShift\n",
    "from chord_recognition.cache import HDF5Cache\n",
    "from chord_recognition.cnn import model\n",
    "from chord_recognition.dataset import ChromaDataset, prepare_datasource, split_datasource, get_weighted_random_sampler\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (14, 5)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datasource = prepare_datasource(('queen', 'beatles', 'robbie_williams', 'zweieck'))\n",
    "datasource = prepare_datasource(('beatles', ))\n",
    "dataset = ChromaDataset(\n",
    "    datasource, window_size=8192, hop_length=4096,\n",
    "    cache=HDF5Cache('chroma_cache.hdf5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train/val keeping equal proportions of the each class\n",
    "# Now train and val datasets have equal class probability distributions.\n",
    "X = [sample for sample, _ in dataset]\n",
    "y = [target for _, target in dataset]\n",
    "indices = np.arange(len(X))\n",
    "X_train, _, y_train, _, idx_train, idx_val = train_test_split(\n",
    "    X, y, indices, test_size=0.2, stratify=y)\n",
    "\n",
    "# Balance the classes in each batch which hopefully helps the training.\n",
    "class_idxs = [x[1] for x in dataset]\n",
    "_, class_counts = np.unique(class_idxs, return_counts=True)\n",
    "sampler = get_weighted_random_sampler(zip(X_train, y_train), class_counts)\n",
    "del X, y, X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(71839, 17960)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idx_train), len(idx_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train/val datasets\n",
    "# Make sure that train and val datasets have equal class probability distributions.\n",
    "loader_train = DataLoader(\n",
    "    dataset=dataset[idx_train],\n",
    "    batch_size=128,\n",
    "    sampler=sampler,\n",
    "    pin_memory=False,\n",
    "    num_workers=0)\n",
    "loader_val = DataLoader(\n",
    "    dataset=dataset[idx_val],\n",
    "    num_workers=0,\n",
    "    batch_size=25)\n",
    "\n",
    "dataloaders = {\n",
    "    \"train\": loader_train,\n",
    "    \"val\": loader_val\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state_dict, is_best, filename='best_model.pt'):\n",
    "    if is_best:\n",
    "        torch.save(state_dict, f'chord_recognition/models/{filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, dataloaders, device, epochs=1):\n",
    "    liveloss = PlotLosses()\n",
    "    model = model.to(device=device)\n",
    "    \n",
    "    best_acc = 0\n",
    "    for e in range(epochs):\n",
    "        logs = {}\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # put model to training mode\n",
    "            else:\n",
    "                model.eval()\n",
    "                \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "        \n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device=device, dtype=torch.float32)\n",
    "                labels = labels.to(device=device, dtype=torch.long)\n",
    "                if phase == 'train':\n",
    "                    # Zero out all of the gradients for the variables which the optimizer\n",
    "                    # will update.\n",
    "                    optimizer.zero_grad()\n",
    "            \n",
    "                scores = model(inputs)\n",
    "                scores = scores.squeeze(3).squeeze(2)                \n",
    "                loss = F.cross_entropy(scores, labels)\n",
    "                \n",
    "                _, preds = torch.max(scores, 1)\n",
    "                running_corrects += torch.sum(preds == labels)\n",
    "\n",
    "        \n",
    "                if phase == 'train':        \n",
    "                    # This is the backwards pass: compute the gradient of the loss with\n",
    "                    # respect to each  parameter of the model.\n",
    "                    loss.backward()\n",
    "        \n",
    "                    # Actually update the parameters of the model using the gradients\n",
    "                    # computed by the backwards pass.\n",
    "                    optimizer.step()\n",
    "            \n",
    "                running_loss += loss.detach() * inputs.size(0)\n",
    "        \n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.float() / len(dataloaders[phase].dataset)\n",
    "            prefix = ''\n",
    "            if phase == 'val':\n",
    "                prefix = 'val_'\n",
    "                is_best = epoch_acc > best_acc\n",
    "                best_acc = max(epoch_acc, best_acc)\n",
    "                save_checkpoint(model.state_dict(), is_best)\n",
    "            \n",
    "            logs[prefix + ' log loss'] = epoch_loss.item()\n",
    "            logs[prefix + 'accuracy'] = epoch_acc.item()\n",
    "        \n",
    "        liveloss.update(logs)\n",
    "        liveloss.send()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 119, 15])\n",
      "tensor([24, 14,  4, 13,  6,  1, 22, 11, 19, 18,  0, 16,  1,  6, 21, 20,  4,  6,\n",
      "         7,  4, 17,  5,  3, 19,  4,  2,  1, 20, 13,  2,  9, 14,  8, 10,  0,  3,\n",
      "         1, 20, 23,  0, 18, 20, 10, 20, 21,  2, 12,  8,  2, 19,  2,  4, 21, 13,\n",
      "         4,  6, 16,  7, 11,  0,  8, 16, 12,  2, 18, 11, 20, 19, 18, 21, 22, 21,\n",
      "        12,  2, 11, 13, 10, 15, 15,  2,  9, 19, 20,  9, 10, 18, 14, 20,  1, 19,\n",
      "        14, 20, 22, 15, 12,  6,  7, 13, 19,  9, 10, 18,  6, 21, 10,  5, 19,  4,\n",
      "        10,  9,  7, 19, 14, 21, 23,  5, 23, 13, 17,  6, 15, 20,  1, 13, 19, 16,\n",
      "        21, 24])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a803ad579415>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbetas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.999\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-37ce81c56ad5>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, dataloaders, device, epochs)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunning_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mepoch_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunning_corrects\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m             \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'val'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'float'"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999))\n",
    "\n",
    "train_model(model, optimizer, dataloaders, device, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminate overfitting:\n",
    "# - make sure that training and validation datasets have equal class probability distributions.\n",
    "# - make sure that batch accepts accepts all classes\n",
    "# - Try CNN -> CTC loss https://www.cs.toronto.edu/~graves/icml_2006.pdf\n",
    "# - try CNN for extraction a sequence of features and vanilla RNN to propagate information through this sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
