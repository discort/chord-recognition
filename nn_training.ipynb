{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy, copy\n",
    "import os\n",
    "import pathlib\n",
    "from random import shuffle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, ConcatDataset, WeightedRandomSampler\n",
    "from livelossplot import PlotLosses\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from chord_recognition.augmentations import SemitoneShift, DetuningShift\n",
    "from chord_recognition.cache import HDF5Cache\n",
    "from chord_recognition.cnn import myauditory as model\n",
    "from chord_recognition.dataset import ChromaDataset, prepare_datasource, get_weighted_random_sampler\n",
    "from chord_recognition.utils import Rescale\n",
    "\n",
    "torch.manual_seed(2020)\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (14, 5)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datasource = prepare_datasource(('queen', 'beatles', 'robbie_williams', 'zweieck'))\n",
    "datasource = prepare_datasource(('beatles',))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ChromaDataset(\n",
    "    datasource, window_size=8192, hop_length=4096,\n",
    "    cache=HDF5Cache('chroma_cache.hdf5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train/val keeping equal proportions of the each class\n",
    "# Now train and val datasets have equal class probability distributions.\n",
    "X = [s for s, _ in dataset]\n",
    "targets = [t for _, t in dataset]\n",
    "indices = np.arange(len(X))\n",
    "X_train, _, y_train, _, idx_train, idx_val = train_test_split(\n",
    "    X, targets, indices, test_size=0.2, stratify=targets, random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate TRAIN_MEAN, TRAIN_STD\n",
    "#X_train = np.hstack([sample.squeeze(0) for sample in X_train])\n",
    "#TRAIN_MEAN = X_train.mean(axis=1)\n",
    "#TRAIN_MEAN\n",
    "#TRAIN_STD = X_train.std(axis=1)\n",
    "#TRAIN_STD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance the classes in each batch which hopefully helps the training.\n",
    "sampler = get_weighted_random_sampler(targets, y_train)\n",
    "del X, targets, X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(195652, 48914)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idx_train), len(idx_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Split dataset into train/val datasets\n",
    "# Make sure that train and val datasets have equal class probability distributions.\n",
    "loader_train = DataLoader(\n",
    "    dataset=dataset[idx_train],\n",
    "    batch_size=batch_size,\n",
    "    sampler=sampler,\n",
    "    pin_memory=False,\n",
    "    num_workers=0)\n",
    "loader_val = DataLoader(\n",
    "    dataset=dataset[idx_val],\n",
    "    num_workers=0,\n",
    "    batch_size=batch_size)\n",
    "\n",
    "dataloaders = {\n",
    "    \"train\": loader_train,\n",
    "    \"val\": loader_val\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state_dict, is_best, filename='best_model.pt'):\n",
    "    if is_best:\n",
    "        torch.save(state_dict, f'chord_recognition/models/{filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, dataloaders, device, epochs=1):\n",
    "    liveloss = PlotLosses()\n",
    "    model = model.to(device=device)\n",
    "    \n",
    "    best_acc = 0\n",
    "    for e in range(epochs):\n",
    "        logs = {}\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # put model to training mode\n",
    "            else:\n",
    "                model.eval()\n",
    "                \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "        \n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device=device, dtype=torch.float32)\n",
    "                labels = labels.to(device=device, dtype=torch.long)\n",
    "                if phase == 'train':\n",
    "                    # Zero out all of the gradients for the variables which the optimizer\n",
    "                    # will update.\n",
    "                    optimizer.zero_grad()\n",
    "            \n",
    "                scores = model(inputs)\n",
    "                scores = scores.squeeze(3).squeeze(2)                \n",
    "                loss = F.cross_entropy(scores, labels)\n",
    "                \n",
    "                _, preds = torch.max(scores, 1)\n",
    "                running_corrects += torch.sum(preds == labels)\n",
    "\n",
    "        \n",
    "                if phase == 'train':        \n",
    "                    # This is the backwards pass: compute the gradient of the loss with\n",
    "                    # respect to each  parameter of the model.\n",
    "                    loss.backward()\n",
    "        \n",
    "                    # Actually update the parameters of the model using the gradients\n",
    "                    # computed by the backwards pass.\n",
    "                    optimizer.step()\n",
    "            \n",
    "                running_loss += loss.detach() * inputs.size(0)\n",
    "        \n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.float() / len(dataloaders[phase].dataset)\n",
    "            prefix = ''\n",
    "            if phase == 'val':\n",
    "                prefix = 'val_'\n",
    "                is_best = epoch_acc > best_acc\n",
    "                best_acc = max(epoch_acc, best_acc)\n",
    "                save_checkpoint(model.state_dict(), is_best)\n",
    "            \n",
    "            logs[prefix + ' log loss'] = epoch_loss.item()\n",
    "            logs[prefix + 'accuracy'] = epoch_acc.item()\n",
    "        \n",
    "        liveloss.update(logs)\n",
    "        liveloss.send()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([22, 12,  5,  1,  7, 11, 11, 19, 12,  9, 12, 21,  9,  7, 21, 20,  1, 23,\n",
      "         7,  4,  5,  2, 11, 15, 21, 24, 19,  8, 23,  0,  7,  3, 20, 17, 21, 12,\n",
      "         6,  9, 13, 20, 11, 16,  9,  8, 18, 10,  9, 18,  3,  0, 18,  5,  2, 10,\n",
      "         5, 20, 19, 14, 16, 12, 19, 17,  9, 20])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-7af7adb728be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbetas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.999\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-27-4fc88e133a4f>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, dataloaders, device, epochs)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunning_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mepoch_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunning_corrects\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'val'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'float'"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-4\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999))\n",
    "\n",
    "train_model(model, optimizer, dataloaders, device, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminate overfitting:\n",
    "# - Try CNN -> CTC loss https://www.cs.toronto.edu/~graves/icml_2006.pdf\n",
    "# - try CNN for extraction a sequence of features and vanilla RNN to propagate information through this sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
